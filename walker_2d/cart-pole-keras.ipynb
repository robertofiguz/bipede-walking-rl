{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import walker\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.95  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 20  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 200\n",
    "\n",
    "env = walker.Walker()\n",
    "\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = 8*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(8, 1)]             0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (8, 128)             256         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (8, 128)             16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (8, 128)             16512       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (8, 128)             16512       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (8, 128)             16512       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (8, 128)             16512       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (8, 128)             16512       ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (8, 128)             16512       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " motor_1 (Dense)                (8, 3)               387         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " motor_2 (Dense)                (8, 3)               387         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " motor_3 (Dense)                (8, 3)               387         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " motor_4 (Dense)                (8, 3)               387         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " motor_5 (Dense)                (8, 3)               387         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " motor_6 (Dense)                (8, 3)               387         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " motor_7 (Dense)                (8, 3)               387         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " motor_8 (Dense)                (8, 3)               387         ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 118,936\n",
      "Trainable params: 118,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 16:33:12.608726: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-22 16:33:12.608902: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def build_model(states):\n",
    "        # Define model layers.\n",
    "    input_layer = Input(1,8)\n",
    "    first_dense = Dense(units='128', activation='relu')(input_layer)\n",
    "    # Y1 output will be fed from the first dense\n",
    "    y1_output = Dense(units='3', name='motor_1')(first_dense)\n",
    "\n",
    "    second_dense = Dense(units='128',activation='relu')(first_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y2_output = Dense(units='3',name='motor_2')(second_dense)\n",
    "\n",
    "    third_dense = Dense(units='128',activation='relu')(second_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y3_output = Dense(units='3',name='motor_3')(third_dense)\n",
    "\n",
    "    four_dense = Dense(units='128',activation='relu')(third_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y4_output = Dense(units='3',name='motor_4')(four_dense)\n",
    "\n",
    "    five_dense = Dense(units='128',activation='relu')(four_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y5_output = Dense(units='3',name='motor_5')(five_dense)\n",
    "\n",
    "    six_dense = Dense(units='128',activation='relu')(five_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y6_output = Dense(units='3',name='motor_6')(six_dense)\n",
    "\n",
    "    seven_dense = Dense(units='128',activation='relu')(six_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y7_output = Dense(units='3',name='motor_7')(seven_dense)\n",
    "\n",
    "    eight_dense = Dense(units='128',activation='relu')(seven_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y8_output = Dense(units='3',name='motor_8')(eight_dense)\n",
    "\n",
    "    # Define the model with the input layer \n",
    "    # and a list of output layers\n",
    "    model = Model(inputs=input_layer,outputs=[y1_output, y2_output, y3_output,y4_output,y5_output,y6_output,y7_output,y8_output])\n",
    "\n",
    "    return model\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = build_model(states)\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = build_model(states)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msudofork\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/Users/roberto/miniforge3/lib/python3.9/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import logger\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "run = wandb.init(\n",
    "    config={\n",
    "        \"gamma\": 0.99, \n",
    "        \"epsilon\": 1,\n",
    "        \"epsilon_min\": 0.1,\n",
    "        \"target_reward\": 400.0,\n",
    "        \"batch_size\": 64,\n",
    "        \"win_trials\": 100,\n",
    "        \"units\": 256,\n",
    "        \"learning_rate\": 0.001\n",
    "        },\n",
    "    project=\"cartpole-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 episodes=500):\n",
    "        \"\"\"DQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            state_space (tensor): state space\n",
    "            action_space (tensor): action space\n",
    "            episodes (int): number of episodes to train\n",
    "        \"\"\"\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # experience buffer\n",
    "        self.memory = []\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = run.config.gamma\n",
    "\n",
    "        # initially 90% exploration, 10% exploitation\n",
    "        self.epsilon = run.config.epsilon\n",
    "        # iteratively applying decay til \n",
    "        # 10% exploration/90% exploitation\n",
    "        self.epsilon_min = run.config.epsilon_min\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** \\\n",
    "                             (1. / float(episodes))\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'dqn_cartpole.h5'\n",
    "        # Q Network for training\n",
    "        n_inputs = state_space.shape[0]\n",
    "        n_outputs = env.observation_space\n",
    "  \n",
    "                \n",
    "        self.q_model = build_model(n_inputs)\n",
    "        self.q_model.compile(loss='mae', optimizer=Adam(learning_rate=run.config.learning_rate))\n",
    "        # target Q Network\n",
    "        self.target_q_model = build_model(n_inputs)\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.update_weights()\n",
    "\n",
    "        self.replay_counter = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"save Q Network params to a file\"\"\"\n",
    "        self.q_model.save_weights(self.weights_file)\n",
    "\n",
    "\n",
    "    def update_weights(self):\n",
    "        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"eps-greedy policy\n",
    "        Return:\n",
    "            action (tensor): action to execute\n",
    "        \"\"\"\n",
    "        run.log({\"epsilon\":self.epsilon})\n",
    "        if np.random.rand() < self.epsilon or episode_count<20:\n",
    "            # explore - do random action\n",
    "            return self.action_space.sample()\n",
    "        # exploit\n",
    "        print(\"\\n\\npredicting\\n\\n\")\n",
    "        q_values = self.q_model.predict(state)\n",
    "        # select the action with max Q-value\n",
    "        action = np.argmax(q_values[0])\n",
    "        return action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"store experiences in the replay buffer\n",
    "        Arguments:\n",
    "            state (tensor): env state\n",
    "            action (tensor): agent action\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        \"\"\"\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory.append(item)\n",
    "\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the \n",
    "            non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        Return:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DQN chooses the max Q value among next actions\n",
    "        # selection and evaluation of action is \n",
    "        # on the target Q Network\n",
    "        # Q_max = max_a' Q_target(s', a')\n",
    "        q_value = np.amax(\\\n",
    "                     self.target_q_model.predict(next_state)[0])\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"experience replay addresses the correlation issue \n",
    "            between samples\n",
    "        Arguments:\n",
    "            batch_size (int): replay buffer batch \n",
    "                sample size\n",
    "        \"\"\"\n",
    "        # sars = state, action, reward, state' (next_state)\n",
    "        sars_batch = random.sample(self.memory, batch_size)\n",
    "        state_batch, q_values_batch = [], []\n",
    "\n",
    "        # fixme: for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            # policy prediction for a given state\n",
    "            q_values = self.q_model.predict(state)\n",
    "            \n",
    "            # get Q_max\n",
    "            q_value = self.get_target_q_value(next_state, reward)\n",
    "\n",
    "            # correction on the Q value for the action used\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "\n",
    "            # collect batch state-q_value mapping\n",
    "            state_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        # train the Q-network\n",
    "        self.q_model.fit(np.array(state_batch),\n",
    "                         np.array(q_values_batch),\n",
    "                         batch_size=batch_size,\n",
    "                         verbose=0,\n",
    "                         epochs=1,\n",
    "                         callbacks=WandbCallback())\n",
    "                \n",
    "\n",
    "        # update exploration-exploitation probability\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # copy new params on old target after \n",
    "        # every 10 training updates\n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"decrease the exploration, increase exploitation\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    # the number of trials without falling over\n",
    "    win_trials = run.config.win_trials\n",
    "    # the CartPole-v0 is considered solved if \n",
    "    # for 100 consecutive trials, he cart pole has not \n",
    "    # fallen over and it has achieved an average \n",
    "    # reward of 195.0 \n",
    "    # a reward of +1 is provided for every timestep \n",
    "    # the pole remains upright\n",
    "    win_reward = { 'CartPole-v1' : run.config.target_reward }\n",
    "\n",
    "    # stores the reward per episode\n",
    "    scores = deque(maxlen=win_trials)\n",
    "    rewards_history_full = []\n",
    "    logger.setLevel(logger.ERROR)\n",
    "    env = walker.Walker()\n",
    "\n",
    "    #env.seed(0)\n",
    "\n",
    "    # instantiate the DQN/DDQN agent\n",
    "\n",
    "    agent = DQNAgent(env.observation_space, env.action_space)\n",
    "\n",
    "    # should be solved in this number of episodes\n",
    "    episode_count = 3000\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    batch_size = run.config.batch_size\n",
    "\n",
    "    # by default, CartPole-v1 has max episode steps = 500\n",
    "    # you can use this to experiment beyond 500\n",
    "    # env._max_episode_steps = 4000\n",
    "\n",
    "    # Q-Learning sampling and fitting\n",
    "    for episode in range(episode_count):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # in CartPole-v0, action=0 is left and action=1 is right\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #env.render()\n",
    "            # in CartPole-v0:\n",
    "            # state = [pos, vel, theta, angular speed]\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # store every experience unit in replay buffer\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards_history_full.append(total_reward)\n",
    "        # call experience relay\n",
    "        if len(agent.memory) >= batch_size:\n",
    "            #agent.replay(batch_size)\n",
    "            pass\n",
    "        scores.append(total_reward)\n",
    "        mean_score = np.mean(scores)\n",
    "        run.log({\"episode_score\": mean_score})\n",
    "        if mean_score >= win_reward[\"CartPole-v1\"] \\\n",
    "                and episode >= win_trials:\n",
    "            print(\"Solved in episode %d: \\\n",
    "                   Mean survival = %0.2lf in %d episodes\"\n",
    "                  % (episode, mean_score, win_trials))\n",
    "            print(\"Epsilon: \", agent.epsilon)\n",
    "            agent.save_weights()\n",
    "            break\n",
    "        if (episode + 1) % win_trials == 0:\n",
    "            print(\"Episode %d: Mean survival = \\\n",
    "                   %0.2lf in %d episodes\" %\n",
    "                  ((episode + 1), mean_score, win_trials))\n",
    "\n",
    "    # close the env and write monitor result info to disk\n",
    "  #  artifact = wandb.Artifact(\"weights_v1\", \"weights\")\n",
    "  #  artifact.add_file(\"dqn_cartpole.h5\")\n",
    "  #  run.log_artifact(artifact)\n",
    "    run.finish()\n",
    "    env.close() "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a131500f6664666dd459dcc63d270fb7b4d21f27357580e425cf19c89539d686"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
